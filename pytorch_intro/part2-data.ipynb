{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c53e47",
   "metadata": {},
   "source": [
    "# Data in a Nueral Network\n",
    "- Using Torch's Built-in Vision Dataset\n",
    "- basically cheating to use built-in data\n",
    "\n",
    "### Importance of Data\n",
    "**Most time will be spent:**\n",
    "- getting data\n",
    "- prepping data\n",
    "- formating/converting data to work with nueral network\n",
    "- batching\n",
    "\n",
    "- torchvision's dataset is already prepared "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d640313-b467-42e0-8d2d-70e74fbf2dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c9a2ab",
   "metadata": {},
   "source": [
    "## Training data vs Testing Data\n",
    "\n",
    "**must use different datasets**\n",
    "- keep separate\n",
    "\n",
    "in order to *validate* your model, you have to have *out-of-sample* testing data.\n",
    "- this is data that has never been seen before\n",
    "- if machine learns to *overfit data*, it will perform very well with *in-sample* data, but terribly with new data\n",
    "- with millions+ tunable paramaters in a nueral network, if you train your model for to long, it can start to overfit data\n",
    "\n",
    "- need to know what stats to watch and how long to train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660f72e3",
   "metadata": {},
   "source": [
    "## Iterating over a dataset\n",
    "\n",
    "- can be tedious\n",
    "- when get used to syntax, iteration should become easier\n",
    "- (EX:) could write custom transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "140bafa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344it [00:02, 4144506.43it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/train-images-idx3-ubyte.gz to MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 7292391.78it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1649664it [00:00, 4123114.55it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5120it [00:00, 14212333.87it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/raw\n",
      "\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/devel/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True, \n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train=True, download=True, \n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4fe697",
   "metadata": {},
   "source": [
    "## Batches of data and Comments on this Specific Dataset - Preparing data for a model\n",
    "\n",
    "- this is a VERY small dataset (abt 116mb)\n",
    "- this definitely fits in ram, and any common gpu \n",
    "- could pass dataset through model in one go\n",
    "\n",
    ">\n",
    "\n",
    "- the above facts are very impractical when upscaling datasets\n",
    "- massive datasets are where deep learning shines\n",
    "- batches are used to iterate through chunks of items in a dataset so can fit in memory\n",
    "\n",
    ">\n",
    "\n",
    "- this is why GPUs with more cuda cores and more ram are favorable\n",
    "- cuda cores = processing capability\n",
    "- ram = batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deb5af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True) # iterating through 10 items at a time\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True) # a common batch size is anywhere from 8-64 (people like to use base 8 numbers for fun)\n",
    "# batch sizes and nuerons per layer are really trial and error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6b828e",
   "metadata": {},
   "source": [
    "## Batch Sizes and Shuffling - Passing data through a model\n",
    "\n",
    "- there are millions+ weights to create a path to desired outcome \n",
    "- model optimizes these wieghts with data\n",
    "- if you pass the whole dataset at once, the machine may learn some generalizations, but will also figure out some wieghts are arbitrary\n",
    "- machine has no way to know if something is gernally true or if it is just true in sample data\n",
    "- optimizations through using passing batches, things that are genrally true tend to stick around, and *overfitment cases* tend to get erased\n",
    ">\n",
    "- you dont want to have the biggest batch size possible, there is normally a sweet spot\n",
    "- you do want batch size to be big, data iteration with larger datasets will be quicker, but it comes down to trial and error\n",
    ">\n",
    "- the name of the game is **finding generalizations**\n",
    "- passing data through a model in series is more likely to result in the model utilizing generalizations that occur towards the end of a dataset\n",
    "- shuffling data while passing it through a model helps combat this, and helps the model to find generalizations in the entire dataset\n",
    "> \n",
    "- if there is a quicker route to get to decreasing loss, the model will take that route\n",
    "- have to constantly think about how one can obfuscate overfitment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533b7eb",
   "metadata": {},
   "source": [
    "## Iterating over our specific dataset\n",
    "\n",
    "- iterates over specified batch\n",
    "- 10 examples of handwritten digits & 10 tensors of the actual output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0299b9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([3, 2, 6, 3, 3, 2, 8, 0, 0, 7])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b663616e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "x, y = data[0][0], data[1][0]\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a75fd7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.imshow(data[0][0]) - This is not a valid image for graphing (see below)\n",
    "plt.imshow(data[0][0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1706bf",
   "metadata": {},
   "source": [
    "**Example of Pytorch's Shaping usage**\n",
    "\n",
    "- if imported this image, and converted to gray scale, it would be a [28, 28], not a [1, 28, 28]\n",
    "- pytorch wants the 1 in front, which a hurdle and why one needs to pay attention for when one wants to feed into a nueral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c93f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1568e0",
   "metadata": {},
   "source": [
    "## Balancing Data\n",
    "\n",
    "- the model looks for the shortest path to decrease loss\n",
    "- the model doesnt know what the lowest loss could be beforehand\n",
    ">\n",
    "- as the optimizer is trying to decrease loss, it does not know how good it can get\n",
    "- its just trying to decrease loss as fast and as easy as possible\n",
    ">\n",
    "- loss is really just measured on the output of a nueral network\n",
    "- if a dataset is not balanced, model can get stuck in a hole of bias for part of a dataset\n",
    "- need a balanced dataset\n",
    ">\n",
    "- there are ways to get around imbalanced datasets by modifying weights of specific classes when calculating loss, but this is EXTREMELY tedious\n",
    "- so just used a dataset that is as balanced as possible (generally same number of occurances for variations in your data)\n",
    ">\n",
    "- (EX of an imbalanced dataset) (faces) 3 smiles, 15 frowns, 52 neutral\n",
    "- (EX of a perfectly balanced dataset) (faces) 15 smiles, 15 frowns, 15 neutral\n",
    "- (EX of a blended balance in a dataset) (faces) 14 smiles, 18 frowns, 16 neural\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b790f4",
   "metadata": {},
   "source": [
    "## Checking balance in our specific datset\n",
    "- count the predictions from the model\n",
    "- in this case, the model predicts hand-drawings as numbers\n",
    "- count how many times a number was predicted from a drawing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a8ce51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    Xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total+=1\n",
    "\n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d80020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i]/total*100}\") # reorganize the prediction count and print as a percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eef292",
   "metadata": {},
   "source": [
    "# Data is more important than the nueral network!!!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
