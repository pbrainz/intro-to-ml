{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f15925",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "- how do we pass labeled data?\n",
    "- how to train the model to be able to recognize what we are passing\n",
    "- get model to the point where it can predict a handwritten digit it has never seen before\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54c3cc1",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "> how wrong is the model?\n",
    "\n",
    "- goal overtime is to have loss decrease\n",
    "- even if output is correct, chances are the model was wrong in some way\n",
    "- could have been 60% confident, so not completely correct "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678334e5",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "> adjust weights based on the loss/gradients\n",
    "\n",
    "- goal is to adjust weights in such a way as to lower loss over time\n",
    "- over time is based on the learning rate we define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f65024df-3eab-4362-a061-125531af244d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1980, -2.4649, -2.3190, -2.2207, -2.2308, -2.2499, -2.4568, -2.2702,\n",
       "         -2.3519, -2.3023]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine code from part2 and part3\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "train = datasets.MNIST(\"\", train=True, download=True, \n",
    "                      transform = transforms.Compose([\n",
    "                          transforms.ToTensor()\n",
    "                      ]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train=True, download=True, \n",
    "                      transform = transforms.Compose([\n",
    "                          transforms.ToTensor()\n",
    "                      ]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64) \n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)  \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "X = torch.rand((28,28))\n",
    "X = X.view(-1, 28*28)\n",
    "\n",
    "\n",
    "output = net(X)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76926c8",
   "metadata": {},
   "source": [
    "## Learning Rate and the optimization curve\n",
    "\n",
    "> Reference: [sentdex - optimization curve visual](https://youtu.be/9j-_dOze4IM?t=352)\n",
    "\n",
    " - learning rate dictates the size of the step taken to get to a place\n",
    " - it is entirely calculable to determine what weights are needed for loss to be 0 (perfect accuracy)\n",
    " - we dont want loss to be 0, because we will just overfit everything\n",
    " >\n",
    " - we use learning rate to tell the optimizer to lower the loss, but only take such size of steps\n",
    " - the changes made to weights that are based on just the data passed should be overwritten\n",
    " - what remains should be the actual general principles \n",
    " >\n",
    " - if the learning rate is to large/quick, it will take steps where it will always overpass what we want\n",
    " - if we take to small of steps, then it will take forever to achieve the desired outcome\n",
    " - there is really no way to know the desired learning rate beforehand\n",
    " > \n",
    " - what we generally do is a *decaying learning rate* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309dff3",
   "metadata": {},
   "source": [
    "## The plan\n",
    "\n",
    "- calculate loss based on the output\n",
    "- apply that back to the eniter network\n",
    "- adjust weights based on loss\n",
    "- repeat on the expectation that loss falls\n",
    "- we dont optimize for accuracy, we optimize for loss\n",
    "- accuracy follows low loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96556db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Managing gradients\n",
    "\n",
    "- before you pass data through your model, you want to net.zero_grad()\n",
    ">\n",
    "- we batch data to decrease training time\n",
    "- we do not want to pass the entire dataset, but there is a law of dimishing returns\n",
    ">\n",
    "- there could be times where your on a weak computer (rpi)\n",
    "- you might only be able to pass through 1 set of features and labels at a time\n",
    "- this makes learning for the model VERY hard\n",
    ">\n",
    "- if you dont zero the gradient, they will continue to be added together\n",
    "- gradients are basically what contain loss, optimizer uses gradients to modigy wieghts\n",
    "\n",
    "## Calculating Loss (2 ways)\n",
    "\n",
    "1. output is vector quantity\n",
    "    - based on [*one-hot vectors*](https://en.wikipedia.org/wiki/One-hot)\n",
    "    - **OneHot vector EX:** [0, 1, 0, 0]\n",
    "    - use mean squared error\n",
    "\n",
    "\n",
    "2. output is scalar quanitity\n",
    "    - nota vector, just one-hot\n",
    "    - use nll_loss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ddf210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3567, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4150, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0427, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# net.parameters() corresponds to everything that is adjustable in NN\n",
    "# EX: can freeze layers and tell optimizer to only adjust weights in certain layers\n",
    "# rn we want to adjust everything\n",
    "\n",
    "# learning rate concerns training times and how well the model willl learn\n",
    "\n",
    "# using the Adam optimizer\n",
    "# first param is what is adjustable\n",
    "# second param is learning rate\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# a full pass through our data is called an epoch\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        # data is a batch of Featuresets and Labels\n",
    "        \n",
    "        # our data is basically a container that contains:\n",
    "        # 10 featuresets (grayscale pixel values) \n",
    "        # 10 labels (the class that says this is a 3, this is a 9, etc..) (identifies prediction)\n",
    "\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1, 28*28))\n",
    "\n",
    "        # calc how wrong we were\n",
    "        loss = F.nll_loss(output, y)\n",
    "\n",
    "        # back propogate the loss\n",
    "        # one of the things pytroch just does for use\n",
    "\n",
    "        # normally we iterate over net.parameters() and distribute however we want\n",
    "        # but pytorch is cool\n",
    "        loss.backward()\n",
    "\n",
    "        # tell to adjust weights\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b4595-0279-4177-b5f1-c9029574b6a6",
   "metadata": {},
   "source": [
    "**^ Loss went down which is a good thing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefdf87a",
   "metadata": {},
   "source": [
    "## Seeing how correct we were (2 things)\n",
    "\n",
    "1. We can tell model to not calc gradients on specific layers\n",
    "\n",
    "\n",
    "2. We can also tell torch to do something with no_grad() {this is out-of sample data, we just want to know how good the model is}\n",
    "\n",
    "All we're saying is:\n",
    "for every prediction made, does is match the target value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a312d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.974\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 784))\n",
    "        # we want to compare the argmax\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                    correct += 1\n",
    "            total += 1\n",
    "            \n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088bed10",
   "metadata": {},
   "source": [
    "**^ we did good: 97.5%**\n",
    "\n",
    "- really easy to miss something small\n",
    "- the model doesn't understan that it should not be using certain information\n",
    "\n",
    "- normally seeing an accuracy this high is a  red flag\n",
    "- especially if the distribution is something like 10 classes\n",
    "\n",
    "- valid for this case, but be weary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a2bc111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# tesing the NN\n",
    "\n",
    "# show the image from the dataset\n",
    "# this is specifically pulling the 3rd item, which is a 7\n",
    "plt.imshow(X[3].view(28,28))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50362dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "# get the model prediction for the item in the dataset\n",
    "print(torch.argmax(net(X[3].view(-1, 784))[0]))\n",
    "\n",
    "# prediction is in the tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1872948",
   "metadata": {},
   "source": [
    "**^ Model guessed 7, which is correct**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e635434",
   "metadata": {},
   "source": [
    "## Moving Forward\n",
    "\n",
    "**This is cool and all but:**\n",
    "\n",
    "- we kinda cheated\n",
    "- used our own dataset\n",
    "- everything was pretty much done for us\n",
    "- like balance issues\n",
    "- normalization issues\n",
    "- images were already grayscaled and scaled on 0-1\n",
    ">\n",
    "- now we're going to use a different dataset\n",
    "- won't be our dataset, but will be raw images\n",
    "- will be building a new, convolutional model \n",
    ">\n",
    "- LOTS MORE TO LEARN"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
