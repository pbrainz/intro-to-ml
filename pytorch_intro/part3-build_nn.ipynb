{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # super runs the initialiation for the parent class we are inheriting from\n",
    "        # reference - https://www.geeksforgeeks.org/python-super/\n",
    "        super().__init__()\n",
    "        # define the fully connected layers to the nn\n",
    "        # input will be a flattened row of our image pixels\n",
    "        # goal is to make 3 hidden layers of 64 neurons \n",
    "        # output will be 64\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        # previous layer has 64 outputs, next layer need 64 inputs\n",
    "        self.fc2 = nn.Linear(64, 64) \n",
    "        # we can make the output whatever we want\n",
    "        # remember there is generally a sweet spot found with trial and error\n",
    "        # lets stick to 64 bc also remember people like to use base 8\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        # our final layer needs to have 10 outputs bc we have 10 classes/categories \n",
    "        # handwritten digits are identified as digits 0-9, thus 10 classes\n",
    "        self.fc4 = nn.Linear(64, 10)  \n",
    "    \n",
    "    # the method of defining how data flows through the network\n",
    "    # can throw logic in between layers like if statements and can come up with very advanced models\n",
    "    # wayyy more challenging\n",
    "    # pytorch is way more simple\n",
    "    # gradients are automatically calculated\n",
    "    # (EX:) first few layers are image processing\n",
    "    #       subsequent layers are more specific to various tasks\n",
    "    def forward(self, x):\n",
    "        # awesomeness of pytorch\n",
    "        # F.relu() (rectified linear) is the activation function & optimizer\n",
    "        # the activation function is whether or not the neuron is firing (like a brain)\n",
    "        # keeps the outputs of the layers from exploding into CRAZY large numbers\n",
    "        # prevent loss explosion\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        # dont want to run relu in output layer\n",
    "        # network doesnt know anything (ver stupid)\n",
    "        # we want a function that constrains to what we want\n",
    "        # we want only one neuron in the output layer to completely fire (0, 1) not (0.22, 0.78)\n",
    "        # we are not trying to make a regression algorithm (can do that but we arent trying to do that here)\n",
    "        # we want a probability distribution on the output\n",
    "        x = self.fc4(x)\n",
    "        # use log_softmax for probability distribution\n",
    "        # pretty much everyone uses relu for optimizer\n",
    "        # but when you have different types of what you want for the output, this is where things change\n",
    "        # softmax generally works best for multi-class\n",
    "        # also include the dimension of the output\n",
    "        # just defines which thing is the probability distribution on a sum of one\n",
    "        # what do we want to sum to one? dim=1 (distrubuting across the output layer tensors)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass data through model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize values\n",
    "X = torch.rand((28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = net(X) !!!\n",
    "# will result in size mismatch\n",
    "# needs to be flattened first\n",
    "\n",
    "# X = X.view(28*28) !!!\n",
    "# will result in dimension out of range\n",
    "\n",
    "# have to format things exactly how the libraries want them\n",
    "\n",
    "# -1 tells to not worry about the size of the array/tensor, expect anything\n",
    "# \n",
    "X = X.view(-1, 28*28)\n",
    "\n",
    "\n",
    "output = net(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3086, -2.4022, -2.3518, -2.4247, -2.2184, -2.2160, -2.1890, -2.4396,\n",
       "         -2.2639, -2.2502]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
